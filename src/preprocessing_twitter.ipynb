{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"twitter_preprocessing.ipynb","provenance":[],"authorship_tag":"ABX9TyPnZsOsOGHdadxxG7Xzcy0F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-18T11:53:04.059832Z","iopub.execute_input":"2021-11-18T11:53:04.060329Z","iopub.status.idle":"2021-11-18T11:53:50.959417Z","shell.execute_reply.started":"2021-11-18T11:53:04.060245Z","shell.execute_reply":"2021-11-18T11:53:50.958442Z"},"trusted":true,"id":"4hCtt_X0Z0lF"},"source":["%%capture\n","!python -m spacy download pt_core_news_lg\n","import spacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-11-18T11:53:50.961441Z","iopub.execute_input":"2021-11-18T11:53:50.961729Z","iopub.status.idle":"2021-11-18T11:53:50.973446Z","shell.execute_reply.started":"2021-11-18T11:53:50.961693Z","shell.execute_reply":"2021-11-18T11:53:50.972745Z"},"trusted":true,"id":"oFZJ71b5Z0lI"},"source":["import numpy as np\n","import pandas as pd\n","from datetime import datetime\n","import re\n","from unidecode import unidecode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-11-18T11:53:50.974941Z","iopub.execute_input":"2021-11-18T11:53:50.975502Z","iopub.status.idle":"2021-11-18T11:53:51.73019Z","shell.execute_reply.started":"2021-11-18T11:53:50.97546Z","shell.execute_reply":"2021-11-18T11:53:51.729384Z"},"trusted":true,"id":"iTjkwcfwZ0lI"},"source":["dateparser = lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.000Z')\n","\n","filename = '../input/twitter-ipca/data_query3.csv'\n","data = pd.read_csv(filename,\n","                   index_col=0,\n","                   lineterminator='\\n',\n","                   parse_dates=['created_at'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-11-18T11:53:51.731919Z","iopub.execute_input":"2021-11-18T11:53:51.732188Z","iopub.status.idle":"2021-11-18T11:53:51.870059Z","shell.execute_reply.started":"2021-11-18T11:53:51.732154Z","shell.execute_reply":"2021-11-18T11:53:51.869335Z"},"trusted":true,"id":"TSgBiJAPZ0lJ"},"source":["data = data[~data['text'].str.lower().str.contains(r'n.o tem pre.o')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"rNuDGvrUZ0lK"},"source":["nlp = spacy.load('pt_core_news_lg')\n","stop_words = ['pra', 'pro', 'ta', 'q', 'd', 'p', 'c', 'n', 'ne', 'vc', 'tb', 'ai', 'so',\n","              'pq', 'qdo', 'ser', 'ver', 'ter', 'vir', 'ir', 'ficar', 'haver','estar']\n","\n","def text_preprocess(text):\n","    # Filters\n","    text = re.sub(r'@[A-Za-z0-9$-_@.&+]+', ' ', text) # usernames\n","    text = re.sub(r'https?://[A-Za-z0-9./]+', ' ', text) # urls\n","    text = text.replace('RT', ' ') # retweet marks\n","    text = text.replace('\\n', ' ') # line-feed marks\n","    \n","    # Spacy pipeline and filters\n","    text = nlp(text)\n","    tokens = [token.lemma_ for token in text if (not token.is_stop) &\n","                                                (not token.is_punct) &\n","                                                (not token.is_space) &\n","                                                (token.is_alpha)]\n","    \n","    # Join, normalize, and remove additional stop words\n","    text = ' '.join(tokens).lower()\n","    text = unidecode(text)\n","    text = ' '.join([word for word in text.split(' ') if word not in stop_words])\n","            \n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"-EHYda3MZ0lL"},"source":["data['processed_text'] = data['text'].apply(text_preprocess)\n","dataset = data.reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-11-17T13:19:58.96956Z","iopub.execute_input":"2021-11-17T13:19:58.969919Z","iopub.status.idle":"2021-11-17T13:20:00.257001Z","shell.execute_reply.started":"2021-11-17T13:19:58.969848Z","shell.execute_reply":"2021-11-17T13:20:00.2556Z"},"trusted":true,"id":"xvDLNu5oZ0lM"},"source":["dataset.to_csv('data_query3_processed.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-11-17T13:28:22.186414Z","iopub.execute_input":"2021-11-17T13:28:22.1867Z","iopub.status.idle":"2021-11-17T13:28:28.962149Z","shell.execute_reply.started":"2021-11-17T13:28:22.186671Z","shell.execute_reply":"2021-11-17T13:28:28.961022Z"},"trusted":true,"id":"39iDJzbLZ0lN"},"source":["# Simple bigrams maker and file writer\n","edges = []\n","for i in range(len(dataset)):\n","    text = dataset.loc[i, 'processed_text']\n","    text = text.split(' ')\n","    time = dataset.loc[i, 'created_at'].isoformat()\n","    #time = time.replace(hour=0, minute=0, second=0)\n","    #time = time.strftime('%Y-%m-%d %H:%M:%S')\n","    for j, word in enumerate(text):\n","        if j < (len(text) - 1):\n","            edges.append([word, text[j+1], time])\n","\n","edges = pd.DataFrame(edges, columns=['source', 'target', 'time'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-11-17T13:28:30.534067Z","iopub.execute_input":"2021-11-17T13:28:30.534627Z"},"trusted":true,"id":"msRpPIosZ0lO"},"source":["edges.to_csv('query3.edges', index=False)"],"execution_count":null,"outputs":[]}]}